import os   
import glob
#numpy to make arrays in python.
import numpy as np   
#JS ObjectNotation is used to store and transfer the data.
import json  
#Python Image library 
import PIL.Image as Image
#to visualize plots of the training.     
import matplotlib.pyplot as plt  
#h5py lets to store huge amount of data and can be retrieved using Numpy.
import h5py
import scipy.io as io
#Import colormap from matplotlib. 
from matplotlib import cm as CM
#scipy for numerical computations.
from scipy import spatial
import scipy
#gaussian filter is used to make the image blur which helps in transition of color in image smoothly.
from scipy.ndimage.filters import gaussian_filter
#import pytorch as it performs well on images.
import torch
#taqadam to show the progress of the the training.
import tqdm as tqdm

# function to create density maps for images
def gaussian_filter_density(gt):
    print (gt.shape)
    density = np.zeros(gt.shape, dtype=np.float32)
    gt_count = np.count_nonzero(gt)
    if gt_count == 0:
        return density

    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))
    leafsize = 2048
    # build kdtree
    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)
    # query kdtree
    distances, locations = tree.query(pts, k=4)

    print ('generate density...')
    for i, pt in enumerate(pts):
        pt2d = np.zeros(gt.shape, dtype=np.float32)
        pt2d[pt[1],pt[0]] = 1.
        if gt_count > 1:
            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1
        else:
            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point
        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')
    print ('done.')
    return density

root = '/media/tma/DATA/NghiaNguyen/Thesis_Crowd_Counting/dataset/UCF-QNRF_ECCV18'
part_B_train = os.path.join(root,'Train','images')
part_B_test = os.path.join(root,'Test','images')
# root = '/media/tma/DATA/NghiaNguyen/Thesis_Crowd_Counting/dataset/ShanghaiTech'
# part_B_train = os.path.join(root,'part_B/train_data','images')
# part_B_test = os.path.join(root,'part_B/test_data','images')
path_sets = [part_B_train, part_B_test]

img_paths = []
for path in path_sets:
    for img_path in glob.glob(os.path.join(path, '*.jpg')):
        img_paths.append(img_path)

for img_path in img_paths:
    print (img_path)
    # mat = io.loadmat(img_path.replace('.jpg','.mat').replace('images','ground-truth').replace('IMG_','GT_IMG_'))
    mat = io.loadmat(img_path.replace('.jpg','_ann.mat').replace('images','ground-truth'))

    
    img= plt.imread(img_path)
    k = np.zeros((img.shape[0],img.shape[1]))
    # gt = mat["image_info"][0,0][0,0][0]
    gt = mat["annPoints"]
    for i in range(0,len(gt)):
        if int(gt[i][1])<img.shape[0] and int(gt[i][0])<img.shape[1]:
            k[int(gt[i][1]),int(gt[i][0])]=1
    k = gaussian_filter_density(k)
    with h5py.File(img_path.replace('.jpg','.h5').replace('images','ground-truth'), 'w') as hf:
            hf['density'] = k
# plt.imshow(Image.open(img_paths[0]))
# gt_file = h5py.File(img_paths[0].replace('.jpg','.h5').replace('images','ground-truth'),'r')
# groundtruth = np.asarray(gt_file['density'])
# np.sum(groundtruth)
